{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PREPROCESSING OF STRING DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOS\n",
    "# NEVER FUCKING RUN THIS CELL AGAIN\n",
    "\n",
    "import tarfile\n",
    "\n",
    "# unzipping into json\n",
    "path = 'C:/Users/suvit/OneDrive - University of Helsinki/Documents/IntroDS/yelp_dataset.tar' # when running, change to own local path\n",
    "tar = tarfile.open(path, 'r')\n",
    "data = tar.extractall('yelp_dataset')\n",
    "tar.close()\n",
    "\n",
    "# SERIOUSLY DON'T RUN THIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving into datasets - business\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "business_all = pd.read_json('yelp_dataset/yelp_academic_dataset_business.json', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UBER SOS\n",
    "# NOW THIS ONE, NEVER, _NEVER_, RUN THIS ON YOUR OWN DEVICE\n",
    "# YOU WILL FUCK IT UP\n",
    "\n",
    "# saving into datasets - review\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "review_all = pd.read_json('yelp_dataset/yelp_academic_dataset_review.json', lines=True)\n",
    "\n",
    "# I MEAN IT, DON'T DO IT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving into datasets - tip\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "tip_all = pd.read_json('yelp_dataset/yelp_academic_dataset_tip.json', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sizing down datasets\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# business: business_id, name, city, latitude, longitude, stars, is_open, attributes, categories\n",
    "business = business_all.drop('address', 'state', 'postal code', 'review_count', 'hours')\n",
    "\n",
    "# review: business_id, stars, text, useful\n",
    "review = review_all.drop('review_id', 'user_id', 'date', 'funny', 'cool')\n",
    "\n",
    "# tip: text, compliment_count, business_id\n",
    "tip = tip_all.drop('date', 'user_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# downcasing\n",
    "\n",
    "import string\n",
    "\n",
    "review['text'] = review['text'].str.lower()\n",
    "tip['text'] = tip['text'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove punctuation\n",
    "\n",
    "review['text'] = review['text'].str.replace(r'[^\\w/s]+', '', regex=True)\n",
    "tip['text'] = tip['text'].str.replace(r'[^\\w\\s]+', '', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove stop-words\n",
    "\n",
    "path = 'C:/Users/suvit/OneDrive - University of Helsinki/Documents/IntroDS/stopwords_en.txt'\n",
    "with open(path) as f: # when running, change to own local path\n",
    "    stopwords = set(f.read().split(\"\\n\"))\n",
    "    \n",
    "review['text'] = review['text'].apply(lambda x: ' '.join([word for word in str(x).split() if word not in (stopwords)]))\n",
    "tip['text'] = tip['text'].apply(lambda x: ' '.join([word for word in str(x).split() if word not in (stopwords)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stemming\n",
    "\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "ss = SnowballStemmer(language='english')\n",
    "review['text'] = review['text'].apply(lambda x: ' '.join([ss.stem(word) for word in str(x).split()]))\n",
    "tip['text'] = tip['text'].apply(lambda x: ' '.join([ss.stem(word) for word in str(x).split()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving preprocessed data into json files\n",
    "\n",
    "# dividing into positive and negative reviews\n",
    "review_pos = pd.DataFrame()\n",
    "review_neg = pd.DataFrame()\n",
    "for row in range(len(review)):\n",
    "    if review.iloc[row]['stars'] > 3:\n",
    "        review_pos.append(review.iloc[row])\n",
    "    elif review.iloc[row]['stars'] <= 3:\n",
    "        review_neg.append(review.iloc[row])\n",
    "\n",
    "with open('business.json', 'w') as f:\n",
    "    f.write(business.to_json(orient=\"records\"))\n",
    "\n",
    "with open('review_pos.json', 'w') as f:\n",
    "    f.write(review_pos.to_json(orient=\"records\"))\n",
    "\n",
    "with open('review_neg.json', 'w') as f:\n",
    "    f.write(review_neg.to_json(orient=\"records\"))\n",
    "\n",
    "with open('tip.json', 'w') as f:\n",
    "    f.write(tip.to_json(orient=\"records\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d58ea69fd4be483072a57c0f3d43c49afe88d95673704247309f5e6f4d8a2dae"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
